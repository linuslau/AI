Microsoft Windows [Version 10.0.22621.3007]
(c) Microsoft Corporation. All rights reserved.

C:\00_iWorkspace\AI\Llama.cpp\llama.cpp>conda activate llama.cpp

(llama.cpp) C:\00_iWorkspace\AI\Llama.cpp\llama.cpp>python convert.py models/Llama-2-7b-chat/
Loading model file models\Llama-2-7b-chat\consolidated.00.pth
params = Params(n_vocab=32000, n_embd=4096, n_layer=32, n_ctx=4096, n_ff=11008, n_head=32, n_head_kv=32, n_experts=None, n_experts_used=None, f_norm_eps=1e-05, rope_scaling_type=None, f_rope_freq_base=None, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=None, path_model=WindowsPath('models/Llama-2-7b-chat'))
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the newbehaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
32000 32000
Vocab info: <VocabLoader with 32000 base tokens and 0 added tokens>
Special vocab info: <SpecialVocab with 0 merges, special tokens {'bos': 1, 'eos': 2}, add special tokens unset>
tok_embeddings.weight                            -> token_embd.weight                        | BF16   | [32000, 4096]
norm.weight                                      -> output_norm.weight                       | BF16   | [4096]
output.weight                                    -> output.weight                            | BF16   | [32000, 4096]
layers.0.attention.wq.weight                     -> blk.0.attn_q.weight                      | BF16   | [4096, 4096]
layers.0.attention.wk.weight                     -> blk.0.attn_k.weight                      | BF16   | [4096, 4096]
layers.0.attention.wv.weight                     -> blk.0.attn_v.weight                      | BF16   | [4096, 4096]
layers.0.attention.wo.weight                     -> blk.0.attn_output.weight                 | BF16   | [4096, 4096]
layers.0.feed_forward.w1.weight                  -> blk.0.ffn_gate.weight                    | BF16   | [11008, 4096]
layers.0.feed_forward.w2.weight                  -> blk.0.ffn_down.weight                    | BF16   | [4096, 11008]
layers.0.feed_forward.w3.weight                  -> blk.0.ffn_up.weight                      | BF16   | [11008, 4096]
layers.0.attention_norm.weight                   -> blk.0.attn_norm.weight                   | BF16   | [4096]
layers.0.ffn_norm.weight                         -> blk.0.ffn_norm.weight                    | BF16   | [4096]
layers.1.attention.wq.weight                     -> blk.1.attn_q.weight                      | BF16   | [4096, 4096]
layers.1.attention.wk.weight                     -> blk.1.attn_k.weight                      | BF16   | [4096, 4096]
layers.1.attention.wv.weight                     -> blk.1.attn_v.weight                      | BF16   | [4096, 4096]
layers.1.attention.wo.weight                     -> blk.1.attn_output.weight                 | BF16   | [4096, 4096]
layers.1.feed_forward.w1.weight                  -> blk.1.ffn_gate.weight                    | BF16   | [11008, 4096]
layers.1.feed_forward.w2.weight                  -> blk.1.ffn_down.weight                    | BF16   | [4096, 11008]
layers.1.feed_forward.w3.weight                  -> blk.1.ffn_up.weight                      | BF16   | [11008, 4096]
layers.1.attention_norm.weight                   -> blk.1.attn_norm.weight                   | BF16   | [4096]
layers.1.ffn_norm.weight                         -> blk.1.ffn_norm.weight                    | BF16   | [4096]
layers.2.attention.wq.weight                     -> blk.2.attn_q.weight                      | BF16   | [4096, 4096]
layers.2.attention.wk.weight                     -> blk.2.attn_k.weight                      | BF16   | [4096, 4096]
layers.2.attention.wv.weight                     -> blk.2.attn_v.weight                      | BF16   | [4096, 4096]
layers.2.attention.wo.weight                     -> blk.2.attn_output.weight                 | BF16   | [4096, 4096]
layers.2.feed_forward.w1.weight                  -> blk.2.ffn_gate.weight                    | BF16   | [11008, 4096]
layers.2.feed_forward.w2.weight                  -> blk.2.ffn_down.weight                    | BF16   | [4096, 11008]
layers.2.feed_forward.w3.weight                  -> blk.2.ffn_up.weight                      | BF16   | [11008, 4096]
layers.2.attention_norm.weight                   -> blk.2.attn_norm.weight                   | BF16   | [4096]
layers.2.ffn_norm.weight                         -> blk.2.ffn_norm.weight                    | BF16   | [4096]
layers.3.attention.wq.weight                     -> blk.3.attn_q.weight                      | BF16   | [4096, 4096]
layers.3.attention.wk.weight                     -> blk.3.attn_k.weight                      | BF16   | [4096, 4096]
layers.3.attention.wv.weight                     -> blk.3.attn_v.weight                      | BF16   | [4096, 4096]
layers.3.attention.wo.weight                     -> blk.3.attn_output.weight                 | BF16   | [4096, 4096]
layers.3.feed_forward.w1.weight                  -> blk.3.ffn_gate.weight                    | BF16   | [11008, 4096]
layers.3.feed_forward.w2.weight                  -> blk.3.ffn_down.weight                    | BF16   | [4096, 11008]
layers.3.feed_forward.w3.weight                  -> blk.3.ffn_up.weight                      | BF16   | [11008, 4096]
layers.3.attention_norm.weight                   -> blk.3.attn_norm.weight                   | BF16   | [4096]
layers.3.ffn_norm.weight                         -> blk.3.ffn_norm.weight                    | BF16   | [4096]
layers.4.attention.wq.weight                     -> blk.4.attn_q.weight                      | BF16   | [4096, 4096]
layers.4.attention.wk.weight                     -> blk.4.attn_k.weight                      | BF16   | [4096, 4096]
layers.4.attention.wv.weight                     -> blk.4.attn_v.weight                      | BF16   | [4096, 4096]
layers.4.attention.wo.weight                     -> blk.4.attn_output.weight                 | BF16   | [4096, 4096]
layers.4.feed_forward.w1.weight                  -> blk.4.ffn_gate.weight                    | BF16   | [11008, 4096]
layers.4.feed_forward.w2.weight                  -> blk.4.ffn_down.weight                    | BF16   | [4096, 11008]
layers.4.feed_forward.w3.weight                  -> blk.4.ffn_up.weight                      | BF16   | [11008, 4096]
layers.4.attention_norm.weight                   -> blk.4.attn_norm.weight                   | BF16   | [4096]
layers.4.ffn_norm.weight                         -> blk.4.ffn_norm.weight                    | BF16   | [4096]
layers.5.attention.wq.weight                     -> blk.5.attn_q.weight                      | BF16   | [4096, 4096]
layers.5.attention.wk.weight                     -> blk.5.attn_k.weight                      | BF16   | [4096, 4096]
layers.5.attention.wv.weight                     -> blk.5.attn_v.weight                      | BF16   | [4096, 4096]
layers.5.attention.wo.weight                     -> blk.5.attn_output.weight                 | BF16   | [4096, 4096]
layers.5.feed_forward.w1.weight                  -> blk.5.ffn_gate.weight                    | BF16   | [11008, 4096]
layers.5.feed_forward.w2.weight                  -> blk.5.ffn_down.weight                    | BF16   | [4096, 11008]
layers.5.feed_forward.w3.weight                  -> blk.5.ffn_up.weight                      | BF16   | [11008, 4096]
layers.5.attention_norm.weight                   -> blk.5.attn_norm.weight                   | BF16   | [4096]
layers.5.ffn_norm.weight                         -> blk.5.ffn_norm.weight                    | BF16   | [4096]
layers.6.attention.wq.weight                     -> blk.6.attn_q.weight                      | BF16   | [4096, 4096]
layers.6.attention.wk.weight                     -> blk.6.attn_k.weight                      | BF16   | [4096, 4096]
layers.6.attention.wv.weight                     -> blk.6.attn_v.weight                      | BF16   | [4096, 4096]
layers.6.attention.wo.weight                     -> blk.6.attn_output.weight                 | BF16   | [4096, 4096]
layers.6.feed_forward.w1.weight                  -> blk.6.ffn_gate.weight                    | BF16   | [11008, 4096]
layers.6.feed_forward.w2.weight                  -> blk.6.ffn_down.weight                    | BF16   | [4096, 11008]
layers.6.feed_forward.w3.weight                  -> blk.6.ffn_up.weight                      | BF16   | [11008, 4096]
layers.6.attention_norm.weight                   -> blk.6.attn_norm.weight                   | BF16   | [4096]
layers.6.ffn_norm.weight                         -> blk.6.ffn_norm.weight                    | BF16   | [4096]
layers.7.attention.wq.weight                     -> blk.7.attn_q.weight                      | BF16   | [4096, 4096]
layers.7.attention.wk.weight                     -> blk.7.attn_k.weight                      | BF16   | [4096, 4096]
layers.7.attention.wv.weight                     -> blk.7.attn_v.weight                      | BF16   | [4096, 4096]
layers.7.attention.wo.weight                     -> blk.7.attn_output.weight                 | BF16   | [4096, 4096]
layers.7.feed_forward.w1.weight                  -> blk.7.ffn_gate.weight                    | BF16   | [11008, 4096]
layers.7.feed_forward.w2.weight                  -> blk.7.ffn_down.weight                    | BF16   | [4096, 11008]
layers.7.feed_forward.w3.weight                  -> blk.7.ffn_up.weight                      | BF16   | [11008, 4096]
layers.7.attention_norm.weight                   -> blk.7.attn_norm.weight                   | BF16   | [4096]
layers.7.ffn_norm.weight                         -> blk.7.ffn_norm.weight                    | BF16   | [4096]
layers.8.attention.wq.weight                     -> blk.8.attn_q.weight                      | BF16   | [4096, 4096]
layers.8.attention.wk.weight                     -> blk.8.attn_k.weight                      | BF16   | [4096, 4096]
layers.8.attention.wv.weight                     -> blk.8.attn_v.weight                      | BF16   | [4096, 4096]
layers.8.attention.wo.weight                     -> blk.8.attn_output.weight                 | BF16   | [4096, 4096]
layers.8.feed_forward.w1.weight                  -> blk.8.ffn_gate.weight                    | BF16   | [11008, 4096]
layers.8.feed_forward.w2.weight                  -> blk.8.ffn_down.weight                    | BF16   | [4096, 11008]
layers.8.feed_forward.w3.weight                  -> blk.8.ffn_up.weight                      | BF16   | [11008, 4096]
layers.8.attention_norm.weight                   -> blk.8.attn_norm.weight                   | BF16   | [4096]
layers.8.ffn_norm.weight                         -> blk.8.ffn_norm.weight                    | BF16   | [4096]
layers.9.attention.wq.weight                     -> blk.9.attn_q.weight                      | BF16   | [4096, 4096]
layers.9.attention.wk.weight                     -> blk.9.attn_k.weight                      | BF16   | [4096, 4096]
layers.9.attention.wv.weight                     -> blk.9.attn_v.weight                      | BF16   | [4096, 4096]
layers.9.attention.wo.weight                     -> blk.9.attn_output.weight                 | BF16   | [4096, 4096]
layers.9.feed_forward.w1.weight                  -> blk.9.ffn_gate.weight                    | BF16   | [11008, 4096]
layers.9.feed_forward.w2.weight                  -> blk.9.ffn_down.weight                    | BF16   | [4096, 11008]
layers.9.feed_forward.w3.weight                  -> blk.9.ffn_up.weight                      | BF16   | [11008, 4096]
layers.9.attention_norm.weight                   -> blk.9.attn_norm.weight                   | BF16   | [4096]
layers.9.ffn_norm.weight                         -> blk.9.ffn_norm.weight                    | BF16   | [4096]
layers.10.attention.wq.weight                    -> blk.10.attn_q.weight                     | BF16   | [4096, 4096]
layers.10.attention.wk.weight                    -> blk.10.attn_k.weight                     | BF16   | [4096, 4096]
layers.10.attention.wv.weight                    -> blk.10.attn_v.weight                     | BF16   | [4096, 4096]
layers.10.attention.wo.weight                    -> blk.10.attn_output.weight                | BF16   | [4096, 4096]
layers.10.feed_forward.w1.weight                 -> blk.10.ffn_gate.weight                   | BF16   | [11008, 4096]
layers.10.feed_forward.w2.weight                 -> blk.10.ffn_down.weight                   | BF16   | [4096, 11008]
layers.10.feed_forward.w3.weight                 -> blk.10.ffn_up.weight                     | BF16   | [11008, 4096]
layers.10.attention_norm.weight                  -> blk.10.attn_norm.weight                  | BF16   | [4096]
layers.10.ffn_norm.weight                        -> blk.10.ffn_norm.weight                   | BF16   | [4096]
layers.11.attention.wq.weight                    -> blk.11.attn_q.weight                     | BF16   | [4096, 4096]
layers.11.attention.wk.weight                    -> blk.11.attn_k.weight                     | BF16   | [4096, 4096]
layers.11.attention.wv.weight                    -> blk.11.attn_v.weight                     | BF16   | [4096, 4096]
layers.11.attention.wo.weight                    -> blk.11.attn_output.weight                | BF16   | [4096, 4096]
layers.11.feed_forward.w1.weight                 -> blk.11.ffn_gate.weight                   | BF16   | [11008, 4096]
layers.11.feed_forward.w2.weight                 -> blk.11.ffn_down.weight                   | BF16   | [4096, 11008]
layers.11.feed_forward.w3.weight                 -> blk.11.ffn_up.weight                     | BF16   | [11008, 4096]
layers.11.attention_norm.weight                  -> blk.11.attn_norm.weight                  | BF16   | [4096]
layers.11.ffn_norm.weight                        -> blk.11.ffn_norm.weight                   | BF16   | [4096]
layers.12.attention.wq.weight                    -> blk.12.attn_q.weight                     | BF16   | [4096, 4096]
layers.12.attention.wk.weight                    -> blk.12.attn_k.weight                     | BF16   | [4096, 4096]
layers.12.attention.wv.weight                    -> blk.12.attn_v.weight                     | BF16   | [4096, 4096]
layers.12.attention.wo.weight                    -> blk.12.attn_output.weight                | BF16   | [4096, 4096]
layers.12.feed_forward.w1.weight                 -> blk.12.ffn_gate.weight                   | BF16   | [11008, 4096]
layers.12.feed_forward.w2.weight                 -> blk.12.ffn_down.weight                   | BF16   | [4096, 11008]
layers.12.feed_forward.w3.weight                 -> blk.12.ffn_up.weight                     | BF16   | [11008, 4096]
layers.12.attention_norm.weight                  -> blk.12.attn_norm.weight                  | BF16   | [4096]
layers.12.ffn_norm.weight                        -> blk.12.ffn_norm.weight                   | BF16   | [4096]
layers.13.attention.wq.weight                    -> blk.13.attn_q.weight                     | BF16   | [4096, 4096]
layers.13.attention.wk.weight                    -> blk.13.attn_k.weight                     | BF16   | [4096, 4096]
layers.13.attention.wv.weight                    -> blk.13.attn_v.weight                     | BF16   | [4096, 4096]
layers.13.attention.wo.weight                    -> blk.13.attn_output.weight                | BF16   | [4096, 4096]
layers.13.feed_forward.w1.weight                 -> blk.13.ffn_gate.weight                   | BF16   | [11008, 4096]
layers.13.feed_forward.w2.weight                 -> blk.13.ffn_down.weight                   | BF16   | [4096, 11008]
layers.13.feed_forward.w3.weight                 -> blk.13.ffn_up.weight                     | BF16   | [11008, 4096]
layers.13.attention_norm.weight                  -> blk.13.attn_norm.weight                  | BF16   | [4096]
layers.13.ffn_norm.weight                        -> blk.13.ffn_norm.weight                   | BF16   | [4096]
layers.14.attention.wq.weight                    -> blk.14.attn_q.weight                     | BF16   | [4096, 4096]
layers.14.attention.wk.weight                    -> blk.14.attn_k.weight                     | BF16   | [4096, 4096]
layers.14.attention.wv.weight                    -> blk.14.attn_v.weight                     | BF16   | [4096, 4096]
layers.14.attention.wo.weight                    -> blk.14.attn_output.weight                | BF16   | [4096, 4096]
layers.14.feed_forward.w1.weight                 -> blk.14.ffn_gate.weight                   | BF16   | [11008, 4096]
layers.14.feed_forward.w2.weight                 -> blk.14.ffn_down.weight                   | BF16   | [4096, 11008]
layers.14.feed_forward.w3.weight                 -> blk.14.ffn_up.weight                     | BF16   | [11008, 4096]
layers.14.attention_norm.weight                  -> blk.14.attn_norm.weight                  | BF16   | [4096]
layers.14.ffn_norm.weight                        -> blk.14.ffn_norm.weight                   | BF16   | [4096]
layers.15.attention.wq.weight                    -> blk.15.attn_q.weight                     | BF16   | [4096, 4096]
layers.15.attention.wk.weight                    -> blk.15.attn_k.weight                     | BF16   | [4096, 4096]
layers.15.attention.wv.weight                    -> blk.15.attn_v.weight                     | BF16   | [4096, 4096]
layers.15.attention.wo.weight                    -> blk.15.attn_output.weight                | BF16   | [4096, 4096]
layers.15.feed_forward.w1.weight                 -> blk.15.ffn_gate.weight                   | BF16   | [11008, 4096]
layers.15.feed_forward.w2.weight                 -> blk.15.ffn_down.weight                   | BF16   | [4096, 11008]
layers.15.feed_forward.w3.weight                 -> blk.15.ffn_up.weight                     | BF16   | [11008, 4096]
layers.15.attention_norm.weight                  -> blk.15.attn_norm.weight                  | BF16   | [4096]
layers.15.ffn_norm.weight                        -> blk.15.ffn_norm.weight                   | BF16   | [4096]
layers.16.attention.wq.weight                    -> blk.16.attn_q.weight                     | BF16   | [4096, 4096]
layers.16.attention.wk.weight                    -> blk.16.attn_k.weight                     | BF16   | [4096, 4096]
layers.16.attention.wv.weight                    -> blk.16.attn_v.weight                     | BF16   | [4096, 4096]
layers.16.attention.wo.weight                    -> blk.16.attn_output.weight                | BF16   | [4096, 4096]
layers.16.feed_forward.w1.weight                 -> blk.16.ffn_gate.weight                   | BF16   | [11008, 4096]
layers.16.feed_forward.w2.weight                 -> blk.16.ffn_down.weight                   | BF16   | [4096, 11008]
layers.16.feed_forward.w3.weight                 -> blk.16.ffn_up.weight                     | BF16   | [11008, 4096]
layers.16.attention_norm.weight                  -> blk.16.attn_norm.weight                  | BF16   | [4096]
layers.16.ffn_norm.weight                        -> blk.16.ffn_norm.weight                   | BF16   | [4096]
layers.17.attention.wq.weight                    -> blk.17.attn_q.weight                     | BF16   | [4096, 4096]
layers.17.attention.wk.weight                    -> blk.17.attn_k.weight                     | BF16   | [4096, 4096]
layers.17.attention.wv.weight                    -> blk.17.attn_v.weight                     | BF16   | [4096, 4096]
layers.17.attention.wo.weight                    -> blk.17.attn_output.weight                | BF16   | [4096, 4096]
layers.17.feed_forward.w1.weight                 -> blk.17.ffn_gate.weight                   | BF16   | [11008, 4096]
layers.17.feed_forward.w2.weight                 -> blk.17.ffn_down.weight                   | BF16   | [4096, 11008]
layers.17.feed_forward.w3.weight                 -> blk.17.ffn_up.weight                     | BF16   | [11008, 4096]
layers.17.attention_norm.weight                  -> blk.17.attn_norm.weight                  | BF16   | [4096]
layers.17.ffn_norm.weight                        -> blk.17.ffn_norm.weight                   | BF16   | [4096]
layers.18.attention.wq.weight                    -> blk.18.attn_q.weight                     | BF16   | [4096, 4096]
layers.18.attention.wk.weight                    -> blk.18.attn_k.weight                     | BF16   | [4096, 4096]
layers.18.attention.wv.weight                    -> blk.18.attn_v.weight                     | BF16   | [4096, 4096]
layers.18.attention.wo.weight                    -> blk.18.attn_output.weight                | BF16   | [4096, 4096]
layers.18.feed_forward.w1.weight                 -> blk.18.ffn_gate.weight                   | BF16   | [11008, 4096]
layers.18.feed_forward.w2.weight                 -> blk.18.ffn_down.weight                   | BF16   | [4096, 11008]
layers.18.feed_forward.w3.weight                 -> blk.18.ffn_up.weight                     | BF16   | [11008, 4096]
layers.18.attention_norm.weight                  -> blk.18.attn_norm.weight                  | BF16   | [4096]
layers.18.ffn_norm.weight                        -> blk.18.ffn_norm.weight                   | BF16   | [4096]
layers.19.attention.wq.weight                    -> blk.19.attn_q.weight                     | BF16   | [4096, 4096]
layers.19.attention.wk.weight                    -> blk.19.attn_k.weight                     | BF16   | [4096, 4096]
layers.19.attention.wv.weight                    -> blk.19.attn_v.weight                     | BF16   | [4096, 4096]
layers.19.attention.wo.weight                    -> blk.19.attn_output.weight                | BF16   | [4096, 4096]
layers.19.feed_forward.w1.weight                 -> blk.19.ffn_gate.weight                   | BF16   | [11008, 4096]
layers.19.feed_forward.w2.weight                 -> blk.19.ffn_down.weight                   | BF16   | [4096, 11008]
layers.19.feed_forward.w3.weight                 -> blk.19.ffn_up.weight                     | BF16   | [11008, 4096]
layers.19.attention_norm.weight                  -> blk.19.attn_norm.weight                  | BF16   | [4096]
layers.19.ffn_norm.weight                        -> blk.19.ffn_norm.weight                   | BF16   | [4096]
layers.20.attention.wq.weight                    -> blk.20.attn_q.weight                     | BF16   | [4096, 4096]
layers.20.attention.wk.weight                    -> blk.20.attn_k.weight                     | BF16   | [4096, 4096]
layers.20.attention.wv.weight                    -> blk.20.attn_v.weight                     | BF16   | [4096, 4096]
layers.20.attention.wo.weight                    -> blk.20.attn_output.weight                | BF16   | [4096, 4096]
layers.20.feed_forward.w1.weight                 -> blk.20.ffn_gate.weight                   | BF16   | [11008, 4096]
layers.20.feed_forward.w2.weight                 -> blk.20.ffn_down.weight                   | BF16   | [4096, 11008]
layers.20.feed_forward.w3.weight                 -> blk.20.ffn_up.weight                     | BF16   | [11008, 4096]
layers.20.attention_norm.weight                  -> blk.20.attn_norm.weight                  | BF16   | [4096]
layers.20.ffn_norm.weight                        -> blk.20.ffn_norm.weight                   | BF16   | [4096]
layers.21.attention.wq.weight                    -> blk.21.attn_q.weight                     | BF16   | [4096, 4096]
layers.21.attention.wk.weight                    -> blk.21.attn_k.weight                     | BF16   | [4096, 4096]
layers.21.attention.wv.weight                    -> blk.21.attn_v.weight                     | BF16   | [4096, 4096]
layers.21.attention.wo.weight                    -> blk.21.attn_output.weight                | BF16   | [4096, 4096]
layers.21.feed_forward.w1.weight                 -> blk.21.ffn_gate.weight                   | BF16   | [11008, 4096]
layers.21.feed_forward.w2.weight                 -> blk.21.ffn_down.weight                   | BF16   | [4096, 11008]
layers.21.feed_forward.w3.weight                 -> blk.21.ffn_up.weight                     | BF16   | [11008, 4096]
layers.21.attention_norm.weight                  -> blk.21.attn_norm.weight                  | BF16   | [4096]
layers.21.ffn_norm.weight                        -> blk.21.ffn_norm.weight                   | BF16   | [4096]
layers.22.attention.wq.weight                    -> blk.22.attn_q.weight                     | BF16   | [4096, 4096]
layers.22.attention.wk.weight                    -> blk.22.attn_k.weight                     | BF16   | [4096, 4096]
layers.22.attention.wv.weight                    -> blk.22.attn_v.weight                     | BF16   | [4096, 4096]
layers.22.attention.wo.weight                    -> blk.22.attn_output.weight                | BF16   | [4096, 4096]
layers.22.feed_forward.w1.weight                 -> blk.22.ffn_gate.weight                   | BF16   | [11008, 4096]
layers.22.feed_forward.w2.weight                 -> blk.22.ffn_down.weight                   | BF16   | [4096, 11008]
layers.22.feed_forward.w3.weight                 -> blk.22.ffn_up.weight                     | BF16   | [11008, 4096]
layers.22.attention_norm.weight                  -> blk.22.attn_norm.weight                  | BF16   | [4096]
layers.22.ffn_norm.weight                        -> blk.22.ffn_norm.weight                   | BF16   | [4096]
layers.23.attention.wq.weight                    -> blk.23.attn_q.weight                     | BF16   | [4096, 4096]
layers.23.attention.wk.weight                    -> blk.23.attn_k.weight                     | BF16   | [4096, 4096]
layers.23.attention.wv.weight                    -> blk.23.attn_v.weight                     | BF16   | [4096, 4096]
layers.23.attention.wo.weight                    -> blk.23.attn_output.weight                | BF16   | [4096, 4096]
layers.23.feed_forward.w1.weight                 -> blk.23.ffn_gate.weight                   | BF16   | [11008, 4096]
layers.23.feed_forward.w2.weight                 -> blk.23.ffn_down.weight                   | BF16   | [4096, 11008]
layers.23.feed_forward.w3.weight                 -> blk.23.ffn_up.weight                     | BF16   | [11008, 4096]
layers.23.attention_norm.weight                  -> blk.23.attn_norm.weight                  | BF16   | [4096]
layers.23.ffn_norm.weight                        -> blk.23.ffn_norm.weight                   | BF16   | [4096]
layers.24.attention.wq.weight                    -> blk.24.attn_q.weight                     | BF16   | [4096, 4096]
layers.24.attention.wk.weight                    -> blk.24.attn_k.weight                     | BF16   | [4096, 4096]
layers.24.attention.wv.weight                    -> blk.24.attn_v.weight                     | BF16   | [4096, 4096]
layers.24.attention.wo.weight                    -> blk.24.attn_output.weight                | BF16   | [4096, 4096]
layers.24.feed_forward.w1.weight                 -> blk.24.ffn_gate.weight                   | BF16   | [11008, 4096]
layers.24.feed_forward.w2.weight                 -> blk.24.ffn_down.weight                   | BF16   | [4096, 11008]
layers.24.feed_forward.w3.weight                 -> blk.24.ffn_up.weight                     | BF16   | [11008, 4096]
layers.24.attention_norm.weight                  -> blk.24.attn_norm.weight                  | BF16   | [4096]
layers.24.ffn_norm.weight                        -> blk.24.ffn_norm.weight                   | BF16   | [4096]
layers.25.attention.wq.weight                    -> blk.25.attn_q.weight                     | BF16   | [4096, 4096]
layers.25.attention.wk.weight                    -> blk.25.attn_k.weight                     | BF16   | [4096, 4096]
layers.25.attention.wv.weight                    -> blk.25.attn_v.weight                     | BF16   | [4096, 4096]
layers.25.attention.wo.weight                    -> blk.25.attn_output.weight                | BF16   | [4096, 4096]
layers.25.feed_forward.w1.weight                 -> blk.25.ffn_gate.weight                   | BF16   | [11008, 4096]
layers.25.feed_forward.w2.weight                 -> blk.25.ffn_down.weight                   | BF16   | [4096, 11008]
layers.25.feed_forward.w3.weight                 -> blk.25.ffn_up.weight                     | BF16   | [11008, 4096]
layers.25.attention_norm.weight                  -> blk.25.attn_norm.weight                  | BF16   | [4096]
layers.25.ffn_norm.weight                        -> blk.25.ffn_norm.weight                   | BF16   | [4096]
layers.26.attention.wq.weight                    -> blk.26.attn_q.weight                     | BF16   | [4096, 4096]
layers.26.attention.wk.weight                    -> blk.26.attn_k.weight                     | BF16   | [4096, 4096]
layers.26.attention.wv.weight                    -> blk.26.attn_v.weight                     | BF16   | [4096, 4096]
layers.26.attention.wo.weight                    -> blk.26.attn_output.weight                | BF16   | [4096, 4096]
layers.26.feed_forward.w1.weight                 -> blk.26.ffn_gate.weight                   | BF16   | [11008, 4096]
layers.26.feed_forward.w2.weight                 -> blk.26.ffn_down.weight                   | BF16   | [4096, 11008]
layers.26.feed_forward.w3.weight                 -> blk.26.ffn_up.weight                     | BF16   | [11008, 4096]
layers.26.attention_norm.weight                  -> blk.26.attn_norm.weight                  | BF16   | [4096]
layers.26.ffn_norm.weight                        -> blk.26.ffn_norm.weight                   | BF16   | [4096]
layers.27.attention.wq.weight                    -> blk.27.attn_q.weight                     | BF16   | [4096, 4096]
layers.27.attention.wk.weight                    -> blk.27.attn_k.weight                     | BF16   | [4096, 4096]
layers.27.attention.wv.weight                    -> blk.27.attn_v.weight                     | BF16   | [4096, 4096]
layers.27.attention.wo.weight                    -> blk.27.attn_output.weight                | BF16   | [4096, 4096]
layers.27.feed_forward.w1.weight                 -> blk.27.ffn_gate.weight                   | BF16   | [11008, 4096]
layers.27.feed_forward.w2.weight                 -> blk.27.ffn_down.weight                   | BF16   | [4096, 11008]
layers.27.feed_forward.w3.weight                 -> blk.27.ffn_up.weight                     | BF16   | [11008, 4096]
layers.27.attention_norm.weight                  -> blk.27.attn_norm.weight                  | BF16   | [4096]
layers.27.ffn_norm.weight                        -> blk.27.ffn_norm.weight                   | BF16   | [4096]
layers.28.attention.wq.weight                    -> blk.28.attn_q.weight                     | BF16   | [4096, 4096]
layers.28.attention.wk.weight                    -> blk.28.attn_k.weight                     | BF16   | [4096, 4096]
layers.28.attention.wv.weight                    -> blk.28.attn_v.weight                     | BF16   | [4096, 4096]
layers.28.attention.wo.weight                    -> blk.28.attn_output.weight                | BF16   | [4096, 4096]
layers.28.feed_forward.w1.weight                 -> blk.28.ffn_gate.weight                   | BF16   | [11008, 4096]
layers.28.feed_forward.w2.weight                 -> blk.28.ffn_down.weight                   | BF16   | [4096, 11008]
layers.28.feed_forward.w3.weight                 -> blk.28.ffn_up.weight                     | BF16   | [11008, 4096]
layers.28.attention_norm.weight                  -> blk.28.attn_norm.weight                  | BF16   | [4096]
layers.28.ffn_norm.weight                        -> blk.28.ffn_norm.weight                   | BF16   | [4096]
layers.29.attention.wq.weight                    -> blk.29.attn_q.weight                     | BF16   | [4096, 4096]
layers.29.attention.wk.weight                    -> blk.29.attn_k.weight                     | BF16   | [4096, 4096]
layers.29.attention.wv.weight                    -> blk.29.attn_v.weight                     | BF16   | [4096, 4096]
layers.29.attention.wo.weight                    -> blk.29.attn_output.weight                | BF16   | [4096, 4096]
layers.29.feed_forward.w1.weight                 -> blk.29.ffn_gate.weight                   | BF16   | [11008, 4096]
layers.29.feed_forward.w2.weight                 -> blk.29.ffn_down.weight                   | BF16   | [4096, 11008]
layers.29.feed_forward.w3.weight                 -> blk.29.ffn_up.weight                     | BF16   | [11008, 4096]
layers.29.attention_norm.weight                  -> blk.29.attn_norm.weight                  | BF16   | [4096]
layers.29.ffn_norm.weight                        -> blk.29.ffn_norm.weight                   | BF16   | [4096]
layers.30.attention.wq.weight                    -> blk.30.attn_q.weight                     | BF16   | [4096, 4096]
layers.30.attention.wk.weight                    -> blk.30.attn_k.weight                     | BF16   | [4096, 4096]
layers.30.attention.wv.weight                    -> blk.30.attn_v.weight                     | BF16   | [4096, 4096]
layers.30.attention.wo.weight                    -> blk.30.attn_output.weight                | BF16   | [4096, 4096]
layers.30.feed_forward.w1.weight                 -> blk.30.ffn_gate.weight                   | BF16   | [11008, 4096]
layers.30.feed_forward.w2.weight                 -> blk.30.ffn_down.weight                   | BF16   | [4096, 11008]
layers.30.feed_forward.w3.weight                 -> blk.30.ffn_up.weight                     | BF16   | [11008, 4096]
layers.30.attention_norm.weight                  -> blk.30.attn_norm.weight                  | BF16   | [4096]
layers.30.ffn_norm.weight                        -> blk.30.ffn_norm.weight                   | BF16   | [4096]
layers.31.attention.wq.weight                    -> blk.31.attn_q.weight                     | BF16   | [4096, 4096]
layers.31.attention.wk.weight                    -> blk.31.attn_k.weight                     | BF16   | [4096, 4096]
layers.31.attention.wv.weight                    -> blk.31.attn_v.weight                     | BF16   | [4096, 4096]
layers.31.attention.wo.weight                    -> blk.31.attn_output.weight                | BF16   | [4096, 4096]
layers.31.feed_forward.w1.weight                 -> blk.31.ffn_gate.weight                   | BF16   | [11008, 4096]
layers.31.feed_forward.w2.weight                 -> blk.31.ffn_down.weight                   | BF16   | [4096, 11008]
layers.31.feed_forward.w3.weight                 -> blk.31.ffn_up.weight                     | BF16   | [11008, 4096]
layers.31.attention_norm.weight                  -> blk.31.attn_norm.weight                  | BF16   | [4096]
layers.31.ffn_norm.weight                        -> blk.31.ffn_norm.weight                   | BF16   | [4096]
skipping tensor rope_freqs
Writing models\Llama-2-7b-chat\ggml-model-f16.gguf, format 1
gguf: This GGUF file is for Little Endian only
gguf: WARNING: Adding merges requested but no merges found, output may be non-functional.
gguf: Setting special token type bos to 1
gguf: Setting special token type eos to 2
[  1/291] Writing tensor token_embd.weight                      | size  32000 x   4096  | type F16  | T+   3
[  2/291] Writing tensor output_norm.weight                     | size   4096           | type F32  | T+   4
[  3/291] Writing tensor output.weight                          | size  32000 x   4096  | type F16  | T+   4
[  4/291] Writing tensor blk.0.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   4
[  5/291] Writing tensor blk.0.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   4
[  6/291] Writing tensor blk.0.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   4
[  7/291] Writing tensor blk.0.attn_output.weight               | size   4096 x   4096  | type F16  | T+   4
[  8/291] Writing tensor blk.0.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   4
[  9/291] Writing tensor blk.0.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   4
[ 10/291] Writing tensor blk.0.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   4
[ 11/291] Writing tensor blk.0.attn_norm.weight                 | size   4096           | type F32  | T+   5
[ 12/291] Writing tensor blk.0.ffn_norm.weight                  | size   4096           | type F32  | T+   5
[ 13/291] Writing tensor blk.1.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   5
[ 14/291] Writing tensor blk.1.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   5
[ 15/291] Writing tensor blk.1.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   5
[ 16/291] Writing tensor blk.1.attn_output.weight               | size   4096 x   4096  | type F16  | T+   5
[ 17/291] Writing tensor blk.1.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   5
[ 18/291] Writing tensor blk.1.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   5
[ 19/291] Writing tensor blk.1.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   5
[ 20/291] Writing tensor blk.1.attn_norm.weight                 | size   4096           | type F32  | T+   5
[ 21/291] Writing tensor blk.1.ffn_norm.weight                  | size   4096           | type F32  | T+   5
[ 22/291] Writing tensor blk.2.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   5
[ 23/291] Writing tensor blk.2.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   6
[ 24/291] Writing tensor blk.2.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   6
[ 25/291] Writing tensor blk.2.attn_output.weight               | size   4096 x   4096  | type F16  | T+   6
[ 26/291] Writing tensor blk.2.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   6
[ 27/291] Writing tensor blk.2.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   6
[ 28/291] Writing tensor blk.2.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   6
[ 29/291] Writing tensor blk.2.attn_norm.weight                 | size   4096           | type F32  | T+   6
[ 30/291] Writing tensor blk.2.ffn_norm.weight                  | size   4096           | type F32  | T+   6
[ 31/291] Writing tensor blk.3.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   6
[ 32/291] Writing tensor blk.3.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   6
[ 33/291] Writing tensor blk.3.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   7
[ 34/291] Writing tensor blk.3.attn_output.weight               | size   4096 x   4096  | type F16  | T+   7
[ 35/291] Writing tensor blk.3.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   7
[ 36/291] Writing tensor blk.3.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   7
[ 37/291] Writing tensor blk.3.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   7
[ 38/291] Writing tensor blk.3.attn_norm.weight                 | size   4096           | type F32  | T+   7
[ 39/291] Writing tensor blk.3.ffn_norm.weight                  | size   4096           | type F32  | T+   7
[ 40/291] Writing tensor blk.4.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   7
[ 41/291] Writing tensor blk.4.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   7
[ 42/291] Writing tensor blk.4.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   7
[ 43/291] Writing tensor blk.4.attn_output.weight               | size   4096 x   4096  | type F16  | T+   8
[ 44/291] Writing tensor blk.4.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   8
[ 45/291] Writing tensor blk.4.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   8
[ 46/291] Writing tensor blk.4.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   8
[ 47/291] Writing tensor blk.4.attn_norm.weight                 | size   4096           | type F32  | T+   8
[ 48/291] Writing tensor blk.4.ffn_norm.weight                  | size   4096           | type F32  | T+   8
[ 49/291] Writing tensor blk.5.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   8
[ 50/291] Writing tensor blk.5.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   8
[ 51/291] Writing tensor blk.5.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   9
[ 52/291] Writing tensor blk.5.attn_output.weight               | size   4096 x   4096  | type F16  | T+   9
[ 53/291] Writing tensor blk.5.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   9
[ 54/291] Writing tensor blk.5.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   9
[ 55/291] Writing tensor blk.5.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   9
[ 56/291] Writing tensor blk.5.attn_norm.weight                 | size   4096           | type F32  | T+   9
[ 57/291] Writing tensor blk.5.ffn_norm.weight                  | size   4096           | type F32  | T+   9
[ 58/291] Writing tensor blk.6.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   9
[ 59/291] Writing tensor blk.6.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   9
[ 60/291] Writing tensor blk.6.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   9
[ 61/291] Writing tensor blk.6.attn_output.weight               | size   4096 x   4096  | type F16  | T+   9
[ 62/291] Writing tensor blk.6.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+  10
[ 63/291] Writing tensor blk.6.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+  10
[ 64/291] Writing tensor blk.6.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+  10
[ 65/291] Writing tensor blk.6.attn_norm.weight                 | size   4096           | type F32  | T+  10
[ 66/291] Writing tensor blk.6.ffn_norm.weight                  | size   4096           | type F32  | T+  10
[ 67/291] Writing tensor blk.7.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  10
[ 68/291] Writing tensor blk.7.attn_k.weight                    | size   4096 x   4096  | type F16  | T+  10
[ 69/291] Writing tensor blk.7.attn_v.weight                    | size   4096 x   4096  | type F16  | T+  10
[ 70/291] Writing tensor blk.7.attn_output.weight               | size   4096 x   4096  | type F16  | T+  10
[ 71/291] Writing tensor blk.7.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+  10
[ 72/291] Writing tensor blk.7.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+  11
[ 73/291] Writing tensor blk.7.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+  11
[ 74/291] Writing tensor blk.7.attn_norm.weight                 | size   4096           | type F32  | T+  11
[ 75/291] Writing tensor blk.7.ffn_norm.weight                  | size   4096           | type F32  | T+  11
[ 76/291] Writing tensor blk.8.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  11
[ 77/291] Writing tensor blk.8.attn_k.weight                    | size   4096 x   4096  | type F16  | T+  11
[ 78/291] Writing tensor blk.8.attn_v.weight                    | size   4096 x   4096  | type F16  | T+  11
[ 79/291] Writing tensor blk.8.attn_output.weight               | size   4096 x   4096  | type F16  | T+  11
[ 80/291] Writing tensor blk.8.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+  12
[ 81/291] Writing tensor blk.8.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+  12
[ 82/291] Writing tensor blk.8.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+  12
[ 83/291] Writing tensor blk.8.attn_norm.weight                 | size   4096           | type F32  | T+  12
[ 84/291] Writing tensor blk.8.ffn_norm.weight                  | size   4096           | type F32  | T+  12
[ 85/291] Writing tensor blk.9.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  12
[ 86/291] Writing tensor blk.9.attn_k.weight                    | size   4096 x   4096  | type F16  | T+  12
[ 87/291] Writing tensor blk.9.attn_v.weight                    | size   4096 x   4096  | type F16  | T+  12
[ 88/291] Writing tensor blk.9.attn_output.weight               | size   4096 x   4096  | type F16  | T+  12
[ 89/291] Writing tensor blk.9.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+  13
[ 90/291] Writing tensor blk.9.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+  13
[ 91/291] Writing tensor blk.9.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+  13
[ 92/291] Writing tensor blk.9.attn_norm.weight                 | size   4096           | type F32  | T+  13
[ 93/291] Writing tensor blk.9.ffn_norm.weight                  | size   4096           | type F32  | T+  13
[ 94/291] Writing tensor blk.10.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  13
[ 95/291] Writing tensor blk.10.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  13
[ 96/291] Writing tensor blk.10.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  13
[ 97/291] Writing tensor blk.10.attn_output.weight              | size   4096 x   4096  | type F16  | T+  14
[ 98/291] Writing tensor blk.10.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  14
[ 99/291] Writing tensor blk.10.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  14
[100/291] Writing tensor blk.10.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  14
[101/291] Writing tensor blk.10.attn_norm.weight                | size   4096           | type F32  | T+  14
[102/291] Writing tensor blk.10.ffn_norm.weight                 | size   4096           | type F32  | T+  14
[103/291] Writing tensor blk.11.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  14
[104/291] Writing tensor blk.11.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  14
[105/291] Writing tensor blk.11.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  15
[106/291] Writing tensor blk.11.attn_output.weight              | size   4096 x   4096  | type F16  | T+  15
[107/291] Writing tensor blk.11.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  15
[108/291] Writing tensor blk.11.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  15
[109/291] Writing tensor blk.11.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  15
[110/291] Writing tensor blk.11.attn_norm.weight                | size   4096           | type F32  | T+  15
[111/291] Writing tensor blk.11.ffn_norm.weight                 | size   4096           | type F32  | T+  15
[112/291] Writing tensor blk.12.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  15
[113/291] Writing tensor blk.12.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  15
[114/291] Writing tensor blk.12.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  15
[115/291] Writing tensor blk.12.attn_output.weight              | size   4096 x   4096  | type F16  | T+  15
[116/291] Writing tensor blk.12.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  16
[117/291] Writing tensor blk.12.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  16
[118/291] Writing tensor blk.12.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  16
[119/291] Writing tensor blk.12.attn_norm.weight                | size   4096           | type F32  | T+  16
[120/291] Writing tensor blk.12.ffn_norm.weight                 | size   4096           | type F32  | T+  16
[121/291] Writing tensor blk.13.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  16
[122/291] Writing tensor blk.13.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  16
[123/291] Writing tensor blk.13.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  16
[124/291] Writing tensor blk.13.attn_output.weight              | size   4096 x   4096  | type F16  | T+  16
[125/291] Writing tensor blk.13.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  17
[126/291] Writing tensor blk.13.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  17
[127/291] Writing tensor blk.13.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  17
[128/291] Writing tensor blk.13.attn_norm.weight                | size   4096           | type F32  | T+  17
[129/291] Writing tensor blk.13.ffn_norm.weight                 | size   4096           | type F32  | T+  17
[130/291] Writing tensor blk.14.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  17
[131/291] Writing tensor blk.14.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  17
[132/291] Writing tensor blk.14.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  17
[133/291] Writing tensor blk.14.attn_output.weight              | size   4096 x   4096  | type F16  | T+  17
[134/291] Writing tensor blk.14.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  18
[135/291] Writing tensor blk.14.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  18
[136/291] Writing tensor blk.14.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  18
[137/291] Writing tensor blk.14.attn_norm.weight                | size   4096           | type F32  | T+  18
[138/291] Writing tensor blk.14.ffn_norm.weight                 | size   4096           | type F32  | T+  18
[139/291] Writing tensor blk.15.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  18
[140/291] Writing tensor blk.15.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  18
[141/291] Writing tensor blk.15.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  18
[142/291] Writing tensor blk.15.attn_output.weight              | size   4096 x   4096  | type F16  | T+  18
[143/291] Writing tensor blk.15.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  19
[144/291] Writing tensor blk.15.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  19
[145/291] Writing tensor blk.15.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  19
[146/291] Writing tensor blk.15.attn_norm.weight                | size   4096           | type F32  | T+  19
[147/291] Writing tensor blk.15.ffn_norm.weight                 | size   4096           | type F32  | T+  19
[148/291] Writing tensor blk.16.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  19
[149/291] Writing tensor blk.16.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  19
[150/291] Writing tensor blk.16.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  19
[151/291] Writing tensor blk.16.attn_output.weight              | size   4096 x   4096  | type F16  | T+  19
[152/291] Writing tensor blk.16.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  19
[153/291] Writing tensor blk.16.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  20
[154/291] Writing tensor blk.16.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  20
[155/291] Writing tensor blk.16.attn_norm.weight                | size   4096           | type F32  | T+  20
[156/291] Writing tensor blk.16.ffn_norm.weight                 | size   4096           | type F32  | T+  20
[157/291] Writing tensor blk.17.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  20
[158/291] Writing tensor blk.17.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  20
[159/291] Writing tensor blk.17.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  20
[160/291] Writing tensor blk.17.attn_output.weight              | size   4096 x   4096  | type F16  | T+  20
[161/291] Writing tensor blk.17.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  20
[162/291] Writing tensor blk.17.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  21
[163/291] Writing tensor blk.17.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  21
[164/291] Writing tensor blk.17.attn_norm.weight                | size   4096           | type F32  | T+  21
[165/291] Writing tensor blk.17.ffn_norm.weight                 | size   4096           | type F32  | T+  21
[166/291] Writing tensor blk.18.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  21
[167/291] Writing tensor blk.18.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  21
[168/291] Writing tensor blk.18.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  21
[169/291] Writing tensor blk.18.attn_output.weight              | size   4096 x   4096  | type F16  | T+  21
[170/291] Writing tensor blk.18.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  21
[171/291] Writing tensor blk.18.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  21
[172/291] Writing tensor blk.18.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  22
[173/291] Writing tensor blk.18.attn_norm.weight                | size   4096           | type F32  | T+  22
[174/291] Writing tensor blk.18.ffn_norm.weight                 | size   4096           | type F32  | T+  22
[175/291] Writing tensor blk.19.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  22
[176/291] Writing tensor blk.19.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  22
[177/291] Writing tensor blk.19.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  22
[178/291] Writing tensor blk.19.attn_output.weight              | size   4096 x   4096  | type F16  | T+  22
[179/291] Writing tensor blk.19.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  22
[180/291] Writing tensor blk.19.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  22
[181/291] Writing tensor blk.19.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  23
[182/291] Writing tensor blk.19.attn_norm.weight                | size   4096           | type F32  | T+  23
[183/291] Writing tensor blk.19.ffn_norm.weight                 | size   4096           | type F32  | T+  23
[184/291] Writing tensor blk.20.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  23
[185/291] Writing tensor blk.20.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  23
[186/291] Writing tensor blk.20.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  23
[187/291] Writing tensor blk.20.attn_output.weight              | size   4096 x   4096  | type F16  | T+  23
[188/291] Writing tensor blk.20.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  23
[189/291] Writing tensor blk.20.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  24
[190/291] Writing tensor blk.20.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  24
[191/291] Writing tensor blk.20.attn_norm.weight                | size   4096           | type F32  | T+  24
[192/291] Writing tensor blk.20.ffn_norm.weight                 | size   4096           | type F32  | T+  24
[193/291] Writing tensor blk.21.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  24
[194/291] Writing tensor blk.21.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  24
[195/291] Writing tensor blk.21.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  24
[196/291] Writing tensor blk.21.attn_output.weight              | size   4096 x   4096  | type F16  | T+  24
[197/291] Writing tensor blk.21.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  24
[198/291] Writing tensor blk.21.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  25
[199/291] Writing tensor blk.21.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  25
[200/291] Writing tensor blk.21.attn_norm.weight                | size   4096           | type F32  | T+  25
[201/291] Writing tensor blk.21.ffn_norm.weight                 | size   4096           | type F32  | T+  25
[202/291] Writing tensor blk.22.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  25
[203/291] Writing tensor blk.22.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  25
[204/291] Writing tensor blk.22.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  25
[205/291] Writing tensor blk.22.attn_output.weight              | size   4096 x   4096  | type F16  | T+  25
[206/291] Writing tensor blk.22.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  26
[207/291] Writing tensor blk.22.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  26
[208/291] Writing tensor blk.22.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  26
[209/291] Writing tensor blk.22.attn_norm.weight                | size   4096           | type F32  | T+  26
[210/291] Writing tensor blk.22.ffn_norm.weight                 | size   4096           | type F32  | T+  26
[211/291] Writing tensor blk.23.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  26
[212/291] Writing tensor blk.23.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  26
[213/291] Writing tensor blk.23.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  26
[214/291] Writing tensor blk.23.attn_output.weight              | size   4096 x   4096  | type F16  | T+  26
[215/291] Writing tensor blk.23.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  27
[216/291] Writing tensor blk.23.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  27
[217/291] Writing tensor blk.23.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  27
[218/291] Writing tensor blk.23.attn_norm.weight                | size   4096           | type F32  | T+  27
[219/291] Writing tensor blk.23.ffn_norm.weight                 | size   4096           | type F32  | T+  27
[220/291] Writing tensor blk.24.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  27
[221/291] Writing tensor blk.24.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  27
[222/291] Writing tensor blk.24.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  27
[223/291] Writing tensor blk.24.attn_output.weight              | size   4096 x   4096  | type F16  | T+  27
[224/291] Writing tensor blk.24.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  28
[225/291] Writing tensor blk.24.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  28
[226/291] Writing tensor blk.24.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  28
[227/291] Writing tensor blk.24.attn_norm.weight                | size   4096           | type F32  | T+  28
[228/291] Writing tensor blk.24.ffn_norm.weight                 | size   4096           | type F32  | T+  28
[229/291] Writing tensor blk.25.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  28
[230/291] Writing tensor blk.25.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  28
[231/291] Writing tensor blk.25.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  28
[232/291] Writing tensor blk.25.attn_output.weight              | size   4096 x   4096  | type F16  | T+  28
[233/291] Writing tensor blk.25.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  29
[234/291] Writing tensor blk.25.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  29
[235/291] Writing tensor blk.25.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  29
[236/291] Writing tensor blk.25.attn_norm.weight                | size   4096           | type F32  | T+  29
[237/291] Writing tensor blk.25.ffn_norm.weight                 | size   4096           | type F32  | T+  29
[238/291] Writing tensor blk.26.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  29
[239/291] Writing tensor blk.26.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  29
[240/291] Writing tensor blk.26.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  30
[241/291] Writing tensor blk.26.attn_output.weight              | size   4096 x   4096  | type F16  | T+  30
[242/291] Writing tensor blk.26.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  30
[243/291] Writing tensor blk.26.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  30
[244/291] Writing tensor blk.26.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  30
[245/291] Writing tensor blk.26.attn_norm.weight                | size   4096           | type F32  | T+  30
[246/291] Writing tensor blk.26.ffn_norm.weight                 | size   4096           | type F32  | T+  30
[247/291] Writing tensor blk.27.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  30
[248/291] Writing tensor blk.27.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  30
[249/291] Writing tensor blk.27.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  31
[250/291] Writing tensor blk.27.attn_output.weight              | size   4096 x   4096  | type F16  | T+  31
[251/291] Writing tensor blk.27.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  31
[252/291] Writing tensor blk.27.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  31
[253/291] Writing tensor blk.27.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  31
[254/291] Writing tensor blk.27.attn_norm.weight                | size   4096           | type F32  | T+  31
[255/291] Writing tensor blk.27.ffn_norm.weight                 | size   4096           | type F32  | T+  31
[256/291] Writing tensor blk.28.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  31
[257/291] Writing tensor blk.28.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  32
[258/291] Writing tensor blk.28.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  32
[259/291] Writing tensor blk.28.attn_output.weight              | size   4096 x   4096  | type F16  | T+  32
[260/291] Writing tensor blk.28.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  32
[261/291] Writing tensor blk.28.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  32
[262/291] Writing tensor blk.28.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  32
[263/291] Writing tensor blk.28.attn_norm.weight                | size   4096           | type F32  | T+  33
[264/291] Writing tensor blk.28.ffn_norm.weight                 | size   4096           | type F32  | T+  33
[265/291] Writing tensor blk.29.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  33
[266/291] Writing tensor blk.29.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  33
[267/291] Writing tensor blk.29.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  33
[268/291] Writing tensor blk.29.attn_output.weight              | size   4096 x   4096  | type F16  | T+  33
[269/291] Writing tensor blk.29.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  33
[270/291] Writing tensor blk.29.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  33
[271/291] Writing tensor blk.29.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  34
[272/291] Writing tensor blk.29.attn_norm.weight                | size   4096           | type F32  | T+  34
[273/291] Writing tensor blk.29.ffn_norm.weight                 | size   4096           | type F32  | T+  34
[274/291] Writing tensor blk.30.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  34
[275/291] Writing tensor blk.30.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  34
[276/291] Writing tensor blk.30.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  34
[277/291] Writing tensor blk.30.attn_output.weight              | size   4096 x   4096  | type F16  | T+  34
[278/291] Writing tensor blk.30.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  34
[279/291] Writing tensor blk.30.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  35
[280/291] Writing tensor blk.30.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  35
[281/291] Writing tensor blk.30.attn_norm.weight                | size   4096           | type F32  | T+  35
[282/291] Writing tensor blk.30.ffn_norm.weight                 | size   4096           | type F32  | T+  35
[283/291] Writing tensor blk.31.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  35
[284/291] Writing tensor blk.31.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  35
[285/291] Writing tensor blk.31.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  35
[286/291] Writing tensor blk.31.attn_output.weight              | size   4096 x   4096  | type F16  | T+  35
[287/291] Writing tensor blk.31.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  35
[288/291] Writing tensor blk.31.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  35
[289/291] Writing tensor blk.31.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  36
[290/291] Writing tensor blk.31.attn_norm.weight                | size   4096           | type F32  | T+  36
[291/291] Writing tensor blk.31.ffn_norm.weight                 | size   4096           | type F32  | T+  36
Wrote models\Llama-2-7b-chat\ggml-model-f16.gguf

(llama.cpp) C:\00_iWorkspace\AI\Llama.cpp\llama.cpp>
